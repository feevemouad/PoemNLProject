{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MOUAD AHM\\anaconda3\\envs\\torchenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MOUAD AHM\\anaconda3\\envs\\torchenv\\lib\\site-packages\\accelerate\\utils\\modeling.py:1365: UserWarning: Current model requires 11535744 bytes of buffer for offloaded layers, which seems does not fit any GPU's remaining memory. If you are experiencing a OOM later, please consider using offload_buffers=True.\n",
      "  warnings.warn(\n",
      "WARNING:root:Some parameters are on the meta device device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|system|>\n",
      "You are a poet who writes poetry based on the title</s>\n",
      "<|user|>\n",
      "write a mini poetry titled \"To become the Elden Lord\"</s>\n",
      "<|assistant|>\n",
      "In the land of Elden,\n",
      "Where the Elden Ring calls to you,\n",
      "Amidst the wild and wondrous land,\n",
      "Awaiting the call of destiny.\n",
      "\n",
      "For in you, the Elden Lord lies,\n",
      "Bathed in the light of the dawn,\n",
      "A gift from the great gods,\n",
      "To carry the world on your shoulders.\n",
      "\n",
      "Lead us, O Elden Lord,\n",
      "To the realm of eternal rest,\n",
      "Where your light shall guide us,\n",
      "And your might shall unite us.\n",
      "\n",
      "For in you, our hearts shall find,\n",
      "A place where no darkness doth lurk,\n",
      "A haven of peace and peace,\n",
      "Where we shall never wander.\n",
      "\n",
      "Let us become the Elden Lord,\n",
      "And embody the light that weighs,\n",
      "Through our deeds and our words,\n",
      "The world shall behold the light\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\"text-generation\", model=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a poet who writes poetry based on the title\",\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": \"write a mini poetry titled \\\"To become the Elden Lord\\\"\"},\n",
    "]\n",
    "prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n",
    "outputs = pipe(prompt, max_new_tokens=100, do_sample=True, temperature=0.7, top_k=50, top_p=0.95)\n",
    "print(outputs[0][\"generated_text\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
